---
title: 'Project 1: Wrangling, Exploration, Visualization'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))
```

## Data Wrangling, Exploration, Visualization

### Tia Frederick. EID: tmf977

####My two datasets are USArrests: Violent Crime Rates by US State and Anscombe: U.S. State Public-School Expenditures. US Arressts contains statistics in arrests per 100,000 residents for assault, murder, and rape in each of the 50 U.S. states in 1973, and the variables are labeled as 'Assault', 'Murder', and 'Rape', respectively. The dataset also provides the percent of the population living in urban areas, labeled as 'UrbanPop'. The Anscombe dataset contains statistics for the U.S. states plus Washington D.C. in 1970 for per-capita education expenditures in dollars ('education' variable), per-capita income ('income' variable), proportion under 18 years old per 1000 residents ('young' variable), and urban proportion per 1000 ('urban' variable). The USArrest dataset has 50 observations and 5 variables, and the Anscombe dataset has 51 observations and 5 variables, and the variable name for the states column is called "X1" for both datasets. I am interested in anaylzing these two datasets to see if there is any correlation between the crime arrests and income in each state. My hypothesis is that the states with the lower per-capita income has the most crime arrests, and the higher per-capita income has the least crime arrests. Therefore, I would expect a negative correlation between these two variables. I would also predict this would be the same for the relationship between crime arrests and education expenditures.   


Paragraph or two introducing your datasets and variables, why they are interesting to you, etc.

```{R}
# read your datasets in here, e.g., with read_csv()
library(tidyverse)

#The USArrests dataset will be labeled as data1
#The USArrests dataset will be labeled as data2
data1 <- read_csv("https://vincentarelbundock.github.io/Rdatasets/csv/carData/Anscombe.csv")
data2 <- read_csv("https://vincentarelbundock.github.io/Rdatasets/csv/datasets/USArrests.csv")
```

#### Tidying: Reshaping

If your datasets are tidy already, demonstrate that you can reshape data with pivot wider/longer here (e.g., untidy and then retidy). Alternatively, it may be easier to wait until the wrangling section so you can reshape your summary statistics. Note here if you are going to do this.

```{R}
# your tidying code (if applicable; can also wait until wrangling section)

#Recall: when your data is tidy, the values of each variable fall in their own column vector. 

#data1 and data2 are already tidy; therefore, I will untidy and then retidy to demonstrate how to reshape data with pivot_wider() and pivot_longer()

#The original datasets:
data1
data2

#Untidy:
data1 <- data1 %>% pivot_wider(names_from = X1, values_from = urban )
data2 <- data2 %>% pivot_wider(names_from = X1, values_from = UrbanPop)
data1
data2

#The datasets are now untidy. Let's demonstrate how to retidy with pivot_longer()

data1 <- data1 %>% pivot_longer(cols = ME:HI, names_to = "X1", values_to = "urban", values_drop_na = TRUE)
data2 <- data2 %>% pivot_longer(cols = Alabama:Wyoming, names_to = "X1", values_to = "UrbanPop", values_drop_na = TRUE)
data1
data2

#Let's move the column with the state names back to the front with the relocate() function:

data1 <- data1 %>% relocate(X1, .before = education)
data2 <- data2 %>% relocate(X1, .before = Murder)
data1
data2

#In data2, 'UrbanPop' is now the last column. The original data2 had 'UrbanPop' as the second to last column and 'Rape' as the last column. However, the re-tidied dataset looks better with 'UrbanPop' as the last column because now the crime columns ('Murder', 'Assault', and 'Rape') are grouped together. 

```

    
#### Joining/Merging

```{R}

#data1 contains data for Washington D.C. and data2 does not. Therefore, I am dropping the 24th row in data1 which contains the data for D.C.
data1 <- data1[!(row.names(data1) %in% "24"),]

#Arrange in alphabetical order
data1 <- data1 %>% arrange(data1)

#Changing the abbreviated state names of data1 to the full names of the states so that it would match data2.
data1[1, 1] <- "Alaska"
data1[2, 1] <- "Alabama"
data1[3, 1] <- "Arkansas"
data1[4,1] <- "Arizona"
data1[5, 1] <- "California"
data1[6, 1] <- "Colorado"
data1[7, 1] <- "Connecticut"
data1[8, 1] <- "Delaware"
data1[9, 1] <- "Florida"
data1[10, 1] <- "Georgia"
data1[11, 1] <- "Hawaii"
data1[12, 1] <- "Idaho"
data1[13, 1] <- "Illinois"
data1[14, 1] <- "Indiana"
data1[15, 1] <- "Iowa"
data1[16, 1] <- "Kansas"
data1[17, 1] <- "Kentucky"
data1[18, 1] <- "Louisiana"
data1[19, 1] <- "Massachusetts"
data1[20, 1] <- "Maryland"
data1[21, 1] <- "Maine"
data1[22, 1] <- "Michigan"
data1[23, 1] <- "Minnesota"
data1[24,1] <- "Missouri"
data1[25,1] <- "Mississippi"
data1[26,1] <- "Montana"
data1[27,1] <- "North Carolina"
data1[28,1] <- "North Dakota"
data1[29,1] <- "Nebraska"
data1[30,1] <- "New Hampshire"
data1[31,1] <- "New Jersey"
data1[32,1] <- "New Mexico"
data1[33,1] <- "Nevada"
data1[34,1] <- "New York"
data1[35, 1] <- "Ohio"
data1[36, 1] <- "Oklahoma"
data1[37, 1] <- "Oregon"
data1[38, 1] <- "Pennsylvania"
data1[39, 1] <- "Rhode Island"
data1[40, 1] <- "South Carolina"
data1[41, 1] <- "South Dakota"
data1[42, 1] <- "Tennessee"
data1[43, 1] <- "Texas"
data1[44, 1] <- "Utah"
data1[45, 1] <- "Virginia"
data1[46, 1] <- "Vermont"
data1[47, 1] <- "Washington"
data1[48, 1] <- "Wisconsin"
data1[49, 1] <- "West Virginia"
data1[50, 1] <- "Wyoming"

#Renaming the "X1" column to "States" in both datasets.
data1 <- rename(data1, States = X1)
data2 <- rename(data2, States = X1)

#Rename "UrbanPop" column to "UrbanPercent" for more clarification and distinction from the "urban" column in data1.
data2 <- rename(data2, "UrbanPercent" = UrbanPop)

#Arrange in alphabetical order again.
data1 <- data1 %>% arrange(data1)

#Joining the two datasets
data3 <- full_join(data1,data2)
data3

#50 observations/rows in data1
nrow(data1)

#50 observations/rows in each data2
nrow(data2)

#50 observations/rows in each data3
nrow(data3)

#5 variables/columns in each data1
ncol(data1)

#5 variables/columns in each data2
ncol(data2)

#9 variables/columns in each data1
ncol(data3)

#The Unique ID's in each dataset
colnames(data1)
colnames(data2)
colnames(data3)
```

First off, I took out Washington D.C. from 'data1' so I could focus only on the 50 states and to merge it with 'data2'. 'data1' also had the state names abbreviated, so I wrote out the full name for each so it would match with 'data2', and then I arranged it in alphabetical order. I also renamed the 'X1' column to 'States' and 'UrbanPop' to 'UrbanPercent' so it would be easier to understand the data. I then used full_join() so each state will have the statistics for 'education', 'income', 'young', 'urban', 'Murder', 'Assault', 'Rape', and 'UrbanPercent'. No rows were dropped in the joined dataset. I named the joined dataset as 'data3' and it has 50 observations/rows and 9 variables/columns. 'data1' has 50 observations/rows and 5 variables/columns and 'data2' has 50 observations/rows and 5 variables/columns. The ID's in 'data1' are "States",  "education", "income", "young", "urban". The ID's in 'data2' are "States",  "Murder"       "Assault", "Rape", "UrbanPercent". The ID's in 'data3' are "States", "Murder", "Assault", "Rape", "UrbanPercent", "education", "income", "young", "urban". The variables "education", "income", "young", "urban" appear in 'data1' and 'data3' but not in 'data2'. The variables "Murder", "Assault", "Rape", "UrbanPercent" appear in 'data2' and 'data3' but not 'data1'. The variable "States" is the only one that is shared between all three datasets. Since 'data1' and 'data2' only had one variable in common, the joined dataset 'data3' has 9 columns. The dimensions of 'data3' is 50 by 9.  

####  Wrangling

```{R}

#Use all six core `dplyr` functions in the service of generating summary tables/statistics 
#filter(), mutate(), arrange(), select(), summarize(), group_by()

#Used mutate() to generate a variable called "Total_Arrests" by adding up the data in the crime columns 
data3 <- data3 %>% mutate(Total_Arrests = select(., Murder, Assault, Rape) %>% rowSums(na.rm = TRUE))

#Used mutate() to generate a variable called "ratio" that is the ratio of total arrests to income in each state.
data3 %>% mutate(ratio = Total_Arrests/income)

#Used filter() to see which states as an urban population greater than 60%
data3 %>% filter(UrbanPercent > 60)

#Used a `stringr` function str_detect() to extract the data for Texas. 
data3 %>% filter(str_detect(States, "Texas"))

#Selected the variables 'States' and 'income' with select() and arranged income in descending order with arrange() to view the states with the highest per capita income to the lowest.
data3 %>% select(States, income) %>% arrange(desc(income))

#Computed summary statistics for each of my variables using `summarize` alone and with `group_by`

#Created a categorical variable called "high_low_UrbanPercent" by dichotomizing a numeric variable ("UrbanPercent"). A high urban percentage is greater than or equal to 60% and a low urban percentage is lower than 60%.
data3$high_low_UrbanPercent <- as.factor(ifelse(data3$UrbanPercent < 60, "low", "high"))
data3

data3 %>% group_by(high_low_UrbanPercent) %>% summarize(max_income = max(income, na.rm=T)) 
data3 %>% group_by(high_low_UrbanPercent) %>% summarize(min_income = min(income, na.rm=T)) 
data3 %>% summarize(mean_income = mean(income, na.rm=T)) 
data3 %>% summarize(sd_income = sd(income, na.rm=T)) 
data3 %>% summarize(median_income = median(income, na.rm=T))


data3 %>% group_by(high_low_UrbanPercent) %>% summarize(max_education = max(education, na.rm=T)) 
data3 %>% group_by(high_low_UrbanPercent) %>% summarize(min_education = min(education, na.rm=T)) 
data3 %>% summarize(mean_education = mean(education, na.rm=T)) 
data3 %>% summarize(sd_education = sd(education, na.rm=T)) 
data3 %>% summarize(median_education = median(education, na.rm=T)) 


data3 %>% group_by(high_low_UrbanPercent) %>% summarize(max_young = max(young, na.rm=T)) 
data3 %>% group_by(high_low_UrbanPercent) %>% summarize(min_young = min(young, na.rm=T)) 
data3 %>% summarize(mean_young = mean(young, na.rm=T)) 
data3 %>% summarize(sd_young = sd(young, na.rm=T)) 
data3 %>% summarize(median_young = median(young, na.rm=T)) 


data3 %>% group_by(high_low_UrbanPercent) %>% summarize(max_urban = max(urban, na.rm=T)) 
data3 %>% group_by(high_low_UrbanPercent) %>% summarize(min_urban = min(urban, na.rm=T)) 
data3 %>% summarize(mean_urban = mean(urban, na.rm=T)) 
data3 %>% summarize(sd_urban = sd(urban, na.rm=T)) 
data3 %>% summarize(median_urban = median(urban, na.rm=T)) 


data3 %>% group_by(high_low_UrbanPercent) %>% summarize(max_Murder = max(Murder, na.rm=T)) 
data3 %>% group_by(high_low_UrbanPercent) %>% summarize(min_Murder = min(Murder, na.rm=T)) 
data3 %>% summarize(mean_Murder = mean(Murder, na.rm=T)) 
data3 %>% summarize(sd_Murder = sd(Murder, na.rm=T)) 
data3 %>% summarize(median_Murder = median(Murder, na.rm=T))


data3 %>% group_by(high_low_UrbanPercent) %>% summarize(max_Assault = max(Assault, na.rm=T)) 
data3 %>% group_by(high_low_UrbanPercent) %>% summarize(min_Assualt = min(Assault, na.rm=T)) 
data3 %>% summarize(mean_Assault = mean(Assault, na.rm=T)) 
data3 %>% summarize(sd_Assault = sd(Assault, na.rm=T)) 
data3 %>% summarize(median_Assault = median(Assault, na.rm=T)) 


data3 %>% group_by(high_low_UrbanPercent) %>% summarize(max_UrbanPercent = max(UrbanPercent, na.rm=T)) 
data3 %>% group_by(high_low_UrbanPercent) %>% summarize(min_UrbanPercent = min(UrbanPercent, na.rm=T)) 
data3 %>% summarize(mean_UrbanPercent = mean(UrbanPercent, na.rm=T)) 
data3 %>% summarize(sd_UrbanPercent = sd(UrbanPercent, na.rm=T)) 
data3 %>% summarize(median_UrbanPercent = median(UrbanPercent, na.rm=T)) 


data3 %>% group_by(high_low_UrbanPercent) %>% summarize(max_Rape = max(Rape, na.rm=T)) 
data3 %>% group_by(high_low_UrbanPercent) %>% summarize(min_Rape = min(Rape, na.rm=T)) 
data3 %>% summarize(mean_Rape = mean(Rape, na.rm=T)) 
data3 %>% summarize(sd_Rape = sd(Rape, na.rm=T)) 
data3 %>% summarize(median_Rape = median(Rape, na.rm=T)) 


#Provided a table of counts for each level for my categorical variable.
data3 %>% group_by(high_low_UrbanPercent) %>% summarize(counts=n())

```

Your discussion of wrangling section here. Feel encouraged to break up into more than once code chunk and discuss each in turn.

Firstly, I used mutate() to generate a variable called "Total_Arrests" by adding up the data in the crime columns 
and used mutate() again to generate a variable called "ratio" that is the ratio of total arrests to income in each state. I used filter() to see which states as an urban population greater than 60%. I also used a `stringr` function called str_detect() to extract the data for Texas, and found the urban percentage for Texas in 1973 was 80%. I then selected the variables 'States' and 'income' with select() and arranged income in descending order with arrange() to view the states with the highest per capita income to the lowest, which showed that Connecticut had the highest per-capita income at 4256 and Mississippi the lowest at 2081. Next, I computed summary statistics for each of my variables using `summarize` alone and with `group_by` and created a categorical variable called "high_low_UrbanPercent" by dichotomizing a numeric variable ("UrbanPercent"). A urban percentage is categorized as high if it is greater than or equal to 60% and a low urban percentage is lower than 60%. One of my interesting finding is that there were 33 states that were categorized has a high urban percentage and 17 states with a low urban percentage.



#### Visualizing

```{R}

ggplot(data = data3, aes(x = UrbanPercent, y = income)) + geom_point() + geom_smooth(method = "lm") + ggtitle("UrbanPercent vs Income") + xlab("UrbanPercent (Percent urban population)") + ylab("Income") + theme_light() + scale_y_continuous(labels=scales::dollar) 

```

There is a positive relationship between the urban percentage and per-capita income. Therefore, this plot depicts that a state with a higher urban percentage has a higher per-capita income.

```{R}
ggplot(data3, aes(x = Total_Arrests)) + geom_bar(aes(y=States), stat="summary", width = 0.5) + scale_x_continuous(breaks=seq(0,400,50)) + theme_bw() + xlab("Total_Arrests (Crimes include Murder, Assault, Rape)") + ggtitle("Total Arrests for Each State")

#ggplot(data3, aes(x = high_low_UrbanPercent, fill=States))+ geom_bar(position="fill") + scale_fill_brewer() 
```

This plot shows a bar graph for each of the 50 states of the total arrests. The plot depicts Florida with the highest number of criminal arrests. This finding is a little humorous to me because today on social media I often come across news articles of weird crimes that were committed in Florida.

```{R}
ggplot(data3, aes(x=education)) + geom_histogram(aes(y=..density..), color="black", fill="white", bins=15) + geom_density(color="blue") + scale_x_continuous(labels=scales::dollar) + xlab('"Education": Per-capita education expenditures, dollars') + ggtitle("Per-capita Education Expenditures Distibution in the U.S.") + theme_dark()
```

This plot is a histogram of the per-capita education expenditures in the U.S. The plot depicts that average amount of per-capita education expenditures is around $200 in the country.






